


# For Data Processing
import numpy as np
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from PIL import Image, ImageEnhance

# For ML Models
from tensorflow import keras
from tensorflow.keras.layers import *
from tensorflow.keras.losses import *
from tensorflow.keras.models import *
from tensorflow.keras.metrics import *
from tensorflow.keras.optimizers import *
from tensorflow.keras.applications import *
from tensorflow.keras.preprocessing.image import load_img

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Model

from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Input, Dense, BatchNormalization
from IPython.core.display import Image

# For Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Miscellaneous
from tqdm import tqdm
import random
import os
import keras 
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from PIL import Image
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# plt.style.use('dark_background')
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder





train_dir = r'E:\Github\Medical-image-analysis-for-brain-tumor-detection\Dataset\Training'
test_dir = r'E:\Github\Medical-image-analysis-for-brain-tumor-detection\Dataset\Testing'

train_paths = []
train_labels = []

for label in os.listdir(train_dir):
    # Construct full path to the label directory
    label_dir = os.path.join(train_dir, label)
    # Iterate through images in each class directory
    for image in os.listdir(label_dir):
        # Construct full path to each image
        image_path = os.path.join(label_dir, image)
        train_paths.append(image_path)
        train_labels.append(label)

# Shuffle the data
train_paths, train_labels = shuffle(train_paths, train_labels)





plt.figure(figsize=(14,6))
colors = ['#4285f4', '#ea4335', '#fbbc05', '#34a853']
plt.rcParams.update({'font.size': 14})
plt.pie([len([x for x in train_labels if x=='pituitary']),
         len([x for x in train_labels if x=='notumor']),
         len([x for x in train_labels if x=='meningioma']),
         len([x for x in train_labels if x=='glioma'])],
        labels=['pituitary','notumor', 'meningioma', 'glioma'],
        colors=colors, autopct='%.1f%%', explode=(0.025,0.025,0.025,0.025),
        startangle=30);





test_paths = []
test_labels = []

for label in os.listdir(test_dir):
    # Construct full path to the label directory
    label_dir = os.path.join(test_dir, label)
    # Iterate through images in each class directory
    for image in os.listdir(label_dir):
        # Construct full path to each image
        image_path = os.path.join(label_dir, image)
        test_paths.append(image_path)
        test_labels.append(label)

# Shuffle the data
test_paths, test_labels = shuffle(test_paths, test_labels)





plt.figure(figsize=(14,6))
colors = ['#4285f4', '#ea4335', '#fbbc05', '#34a853']
plt.rcParams.update({'font.size': 14})
plt.pie([len(train_labels), len(test_labels)],
        labels=['Train','Test'],
        colors=colors, autopct='%.1f%%', explode=(0.05,0),
        startangle=30);





def augment_image(image):
    image = Image.fromarray(np.uint8(image))
    image = ImageEnhance.Brightness(image).enhance(random.uniform(0.8,1.2))
    image = ImageEnhance.Contrast(image).enhance(random.uniform(0.8,1.2))
    image = np.array(image)/255.0
    return image


IMAGE_SIZE = 128

def open_images(paths):
    '''
    Given a list of paths to images, this function returns the images as arrays (after augmenting them)
    '''
    images = []
    for path in paths:
        image = load_img(path, target_size=(IMAGE_SIZE,IMAGE_SIZE))
        image = augment_image(image)
        images.append(image)
    return np.array(images)

images = open_images(train_paths[50:59])
labels = train_labels[50:59]
fig = plt.figure(figsize=(12, 6))
for x in range(1, 9):
    fig.add_subplot(2, 4, x)
    plt.axis('off')
    plt.title(labels[x])
    plt.imshow(images[x])
plt.rcParams.update({'font.size': 12})
plt.show()





unique_labels = os.listdir(train_dir)

def encode_label(labels):
    encoded = []
    for x in labels:
        encoded.append(unique_labels.index(x))
    return np.array(encoded)

def decode_label(labels):
    decoded = []
    for x in labels:
        decoded.append(unique_labels[x])
    return np.array(decoded)

def datagen(paths, labels, batch_size=12, epochs=1):
    for _ in range(epochs):
        for x in range(0, len(paths), batch_size):
            batch_paths = paths[x:x+batch_size]
            batch_images = open_images(batch_paths)
            batch_labels = labels[x:x+batch_size]
            batch_labels = encode_label(batch_labels)
            yield batch_images, batch_labels





base_model = VGG16(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3), include_top=False, weights='imagenet')
# Set all layers to non-trainable
for layer in base_model.layers:
    layer.trainable = False
# Set the last vgg block to trainable
base_model.layers[-2].trainable = True
base_model.layers[-3].trainable = True
base_model.layers[-4].trainable = True

model = Sequential()
model.add(Input(shape=(IMAGE_SIZE,IMAGE_SIZE,3)))
model.add(base_model)
model.add(Flatten())
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(len(unique_labels), activation='softmax'))





model.summary()





model.compile(optimizer=Adam(learning_rate=0.0001),
             loss='sparse_categorical_crossentropy',
             metrics=['sparse_categorical_accuracy'])





batch_size = 32
steps = int(len(train_paths)/batch_size)
epochs = 8
history = model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),
                    epochs=epochs, steps_per_epoch=steps)





plt.figure(figsize=(8, 4))
plt.grid(True)
plt.plot(history.history['sparse_categorical_accuracy'], 'b-', linewidth=2, label='Accuracy')
plt.plot(history.history['loss'], 'r-', linewidth=2, label='Loss')
plt.title('Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.xticks(range(len(history.history['sparse_categorical_accuracy'])))
plt.legend(loc='upper right')
plt.show()





batch_size = 32
steps = int(len(test_paths)/batch_size)
y_pred = []
y_true = []
for x,y in tqdm(datagen(test_paths, test_labels, batch_size=batch_size, epochs=1), total=steps):
    pred = model.predict(x)
    pred = np.argmax(pred, axis=-1)
    for i in decode_label(pred):
        y_pred.append(i)
    for i in decode_label(y):
        y_true.append(i)





print(classification_report(y_true, y_pred))





def names(number):
    if number==0:
        return 'No, Its not a tumor'
    else:
        return 'Its a Tumor'


from PIL import Image


from matplotlib.pyplot import imshow
img = Image.open(r"E:\Github\Medical-image-analysis-for-brain-tumor-detection\Dataset\Testing\pituitary\Te-pi_0025.jpg")
x = np.array(img.resize((128,128)))
x = x.reshape(1,128,128,3)
res = model.predict_on_batch(x)
classification = np.where(res == np.amax(res))[1][0]
imshow(img)
print(str(res[0][classification]*100) + '% Conclusion: ' + names(classification))


from matplotlib.pyplot import imshow
img = Image.open(r"E:\Github\Medical-image-analysis-for-brain-tumor-detection\Dataset\Testing\meningioma\Te-me_0014.jpg")
x = np.array(img.resize((128,128)))
x = x.reshape(1,128,128,3)
res = model.predict_on_batch(x)
classification = np.where(res == np.amax(res))[1][0]
imshow(img)
print(str(res[0][classification]*100) + '% Conclusion: ' + names(classification))


def names(number):
    if number==0:
        return 'Its a Tumor'
    else:
        return 'No, Its not a tumor'   
from matplotlib.pyplot import imshow
img = Image.open(r"E:\Github\Medical-image-analysis-for-brain-tumor-detection\Dataset\Testing\notumor\Te-no_0016.jpg")
x = np.array(img.resize((128,128)))
x = x.reshape(1,128,128,3)
res = model.predict_on_batch(x)
classification = np.where(res == np.amax(res))[1][0]
imshow(img)
print(str(res[0][classification]*100) + '% Conclusion: ' + names(classification))





import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Input
from tensorflow.keras.optimizers import Adam


# Define the CNN model
cnn_model = Sequential()
cnn_model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))

# Add convolutional and pooling layers
cnn_model.add(Conv2D(32, (3, 3), activation='relu'))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(64, (3, 3), activation='relu'))
cnn_model.add(MaxPooling2D((2, 2)))

cnn_model.add(Conv2D(128, (3, 3), activation='relu'))
cnn_model.add(MaxPooling2D((2, 2)))

# Add fully connected layers
cnn_model.add(Flatten())
cnn_model.add(Dropout(0.3))
cnn_model.add(Dense(128, activation='relu'))
cnn_model.add(Dropout(0.2))
cnn_model.add(Dense(len(unique_labels), activation='softmax'))

cnn_model.summary()


# Compile the model
cnn_model.compile(optimizer=Adam(learning_rate=0.0001),
                  loss='sparse_categorical_crossentropy',
                  metrics=['sparse_categorical_accuracy'])


# Train the model
batch_size = 32
steps = int(len(train_paths) / batch_size)
epochs = 10
history_1 = cnn_model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),
                        epochs=epochs, steps_per_epoch=steps)


# Create a plot of the training history
plt.figure(figsize=(8, 4))
plt.grid(True)
plt.plot(history_1.history['sparse_categorical_accuracy'], 'b-', linewidth=2, label='Accuracy')
plt.plot(history_1.history['loss'], 'r-', linewidth=2, label='Loss')
plt.title('CNN Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.xticks(range(len(history_1.history['sparse_categorical_accuracy'])))
plt.legend(loc='upper right')
plt.show()


# Evaluate the model
batch_size = 32
steps = int(len(test_paths) / batch_size)
y_pred_1 = []
y_true_1 = []
for x, y in tqdm(datagen(test_paths, test_labels, batch_size=batch_size, epochs=1), total=steps):
    pred_1 = cnn_model.predict(x)
    pred_1 = np.argmax(pred_1, axis=-1)
    y_pred_1.extend(decode_label(pred_1))
    y_true_1.extend(decode_label(y))


# Classification report
print(classification_report(y_true_1, y_pred_1))





from tensorflow.keras.applications import DenseNet121


# Load the DenseNet121 model without the top layers
base_model_2 = DenseNet121(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet')

# Set all layers to non-trainable
for layer in base_model_2.layers:
    layer.trainable = False



# Set some layers to trainable if needed (example here, modify as necessary)
base_model_2.layers[-2].trainable = True
base_model_2.layers[-3].trainable = True
base_model_2.layers[-4].trainable = True


# Create the full model
densenet_model = Sequential()
densenet_model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))
densenet_model.add(base_model_2)
densenet_model.add(Flatten())
densenet_model.add(Dropout(0.3))
densenet_model.add(Dense(128, activation='relu'))
densenet_model.add(Dropout(0.2))
densenet_model.add(Dense(len(unique_labels), activation='softmax'))

densenet_model.summary()


# Compile the model
densenet_model.compile(optimizer=Adam(learning_rate=0.0001),
                       loss='sparse_categorical_crossentropy',
                       metrics=['sparse_categorical_accuracy'])



# Train the model
batch_size = 32
steps_2 = int(len(train_paths) / batch_size)
epochs = 10

history_2 = densenet_model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),
                             epochs=epochs, steps_per_epoch=steps_2)


# Create a plot of the training history
plt.figure(figsize=(8, 4))
plt.grid(True)
plt.plot(history_2.history['sparse_categorical_accuracy'], 'b-', linewidth=2, label='Accuracy')
plt.plot(history_2.history['loss'], 'r-', linewidth=2, label='Loss')
plt.title('DenseNet Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.xticks(range(len(history_2.history['sparse_categorical_accuracy'])))
plt.legend(loc='upper right')
plt.show()


# Evaluate the model
batch_size = 32
steps = int(len(test_paths) / batch_size)
y_pred_2 = []
y_true_2 = []
for x, y in tqdm(datagen(test_paths, test_labels, batch_size=batch_size, epochs=1), total=steps):
    pred_2 = densenet_model.predict(x)
    pred_2 = np.argmax(pred_2, axis=-1)
    y_pred_2.extend(decode_label(pred_2))
    y_true_2.extend(decode_label(y))


# Classification report
print(classification_report(y_true_2, y_pred_2))





from tensorflow.keras.applications import ResNet50


# Load the ResNet50 model without the top layers
base_model_3 = ResNet50(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet')

# Set all layers to non-trainable
for layer in base_model_3.layers:
    layer.trainable = False


# Set the last few layers to trainable
base_model_3.layers[-2].trainable = True
base_model_3.layers[-3].trainable = True
base_model_3.layers[-4].trainable = True


# Create the full model
model = Sequential()
model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))
model.add(base_model_3)
model.add(Flatten())
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(len(unique_labels), activation='softmax'))

model.summary()


# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])


# Train the model
batch_size = 32
steps_4 = int(len(train_paths) / batch_size)
epochs = 10
history_4 = model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),
                    epochs=epochs, steps_per_epoch=steps_4)


# Create a plot of the training history
plt.figure(figsize=(8, 4))
plt.grid(True)
plt.plot(history_4.history['sparse_categorical_accuracy'], 'b-', linewidth=2, label='Accuracy')
plt.plot(history_4.history['loss'], 'r-', linewidth=2, label='Loss')
plt.title('RestNet50 Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.xticks(range(len(history_4.history['sparse_categorical_accuracy'])))
plt.legend(loc='upper right')
plt.show()


# Evaluate the model
batch_size = 32
steps_4 = int(len(test_paths) / batch_size)
y_pred_3 = []
y_true_3 = []
for x, y in tqdm(datagen(test_paths, test_labels, batch_size=batch_size, epochs=1), total=steps_4):
    pred_3 = model.predict(x)
    pred_3 = np.argmax(pred_3, axis=-1)
    y_pred_3.extend(decode_label(pred_3))
    y_true_3.extend(decode_label(y))


# Classification report
print(classification_report(y_true_3, y_pred_3))





from tensorflow.keras.applications import MobileNetV2


# Load the MobileNetV2 model without the top layers
base_model_4 = MobileNetV2(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet')

# Set all layers to non-trainable
for layer in base_model_4.layers:
    layer.trainable = False


# Set the last few layers to trainable
base_model_4.layers[-2].trainable = True
base_model_4.layers[-3].trainable = True
base_model_4.layers[-4].trainable = True


# Create the full model
model = Sequential()
model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))
model.add(base_model_4)
model.add(Flatten())
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(len(unique_labels), activation='softmax'))

model.summary()


# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])



# Train the model
batch_size = 20
steps_5 = int(len(train_paths) / batch_size)
epochs = 10
history_5 = model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),
                    epochs=epochs, steps_per_epoch=steps_5)



# Create a plot of the training history
plt.figure(figsize=(8, 4))
plt.grid(True)
plt.plot(history_5.history['sparse_categorical_accuracy'], 'b-', linewidth=2, label='Accuracy')
plt.plot(history_5.history['loss'], 'r-', linewidth=2, label='Loss')
plt.title('MobileNetv2 Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.xticks(range(len(history_5.history['sparse_categorical_accuracy'])))
plt.legend(loc='upper right')
plt.show()


# Evaluate the model
batch_size = 32
steps_5 = int(len(test_paths) / batch_size)
y_pred_4 = []
y_true_4 = []
for x, y in tqdm(datagen(test_paths, test_labels, batch_size=batch_size, epochs=1), total=steps_5):
    pred_4 = model.predict(x)
    pred_4 = np.argmax(pred_4, axis=-1)
    y_pred_4.extend(decode_label(pred_4))
    y_true_4.extend(decode_label(y))


# Classification report
print(classification_report(y_true_4, y_pred_4))





from tensorflow.keras.applications import VGG19


# Load the base model
base_model_5 = VGG19(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet')

# Set all layers to non-trainable
for layer in base_model_5.layers:
    layer.trainable = False


# Set the last VGG block to trainable
base_model_5.layers[-2].trainable = True
base_model_5.layers[-3].trainable = True
base_model_5.layers[-4].trainable = True


# Build the model
model = Sequential()
model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))
model.add(base_model_5)
model.add(Flatten())
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(len(unique_labels), activation='softmax'))
model.summary()


# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])


# Train the model
batch_size = 32
steps_6 = int(len(train_paths) / batch_size)
epochs = 10
history_6 = model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),
                    epochs=epochs, steps_per_epoch=steps_6)



# Plot training history
plt.figure(figsize=(8, 4))
plt.grid(True)
plt.plot(history_6.history['sparse_categorical_accuracy'], 'b-', linewidth=2, label='Accuracy')
plt.plot(history_6.history['loss'], 'r-', linewidth=2, label='Loss')
plt.title('VGG19 Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.xticks(range(len(history_6.history['sparse_categorical_accuracy'])))
plt.legend(loc='upper right')
plt.show()



# Evaluate the model on the test set
batch_size = 32
steps = int(len(test_paths) / batch_size)
y_pred_5 = []
y_true_5 = []
for x, y in tqdm(datagen(test_paths, test_labels, batch_size=batch_size, epochs=1), total=steps):
    pred_5 = model.predict(x)
    pred_5 = np.argmax(pred_5, axis=-1)
    for i in decode_label(pred_5):
        y_pred_5.append(i)
    for i in decode_label(y):
        y_true_5.append(i)


# Classification report
print(classification_report(y_true_5, y_pred_5))





from tensorflow.keras.applications import EfficientNetB0


# Load the base model
base_model_6 = EfficientNetB0(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet')

# Set all layers to non-trainable
for layer in base_model_6.layers:
    layer.trainable = False


# Set the last few blocks to trainable
for layer in base_model_6.layers[-20:]:
    layer.trainable = True


# Build the model
model = Sequential()
model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))
model.add(base_model_6)
model.add(Flatten())
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(len(unique_labels), activation='softmax'))
model.summary()


# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])


# Train the model
batch_size = 32
steps_7 = int(len(train_paths) / batch_size)
epochs = 8
history_7 = model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),
                    epochs=epochs, steps_per_epoch=steps_7)


# Plot training history
plt.figure(figsize=(8, 4))
plt.grid(True)
plt.plot(history_7.history['sparse_categorical_accuracy'], 'b-', linewidth=2, label='Accuracy')
plt.plot(history_7.history['loss'], 'r-', linewidth=2, label='Loss')
plt.title('EfficientNetB0 Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.xticks(range(len(history_7.history['sparse_categorical_accuracy'])))
plt.legend(loc='upper right')
plt.show()



# Evaluate the model on the test set
batch_size = 32
steps_7 = int(len(test_paths) / batch_size)
y_pred_6 = []
y_true_6 = []
for x, y in tqdm(datagen(test_paths, test_labels, batch_size=batch_size, epochs=1), total=steps_7):
    pred_6 = model.predict(x)
    pred_6 = np.argmax(pred_6, axis=-1)
    for i in decode_label(pred_6):
        y_pred_6.append(i)
    for i in decode_label(y):
        y_true_6.append(i)


# Classification report
print(classification_report(y_true_6, y_pred_6))








from tensorflow.keras.applications import Xception


# Load the base model
base_model_7 = Xception(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet')

# Freeze all layers
for layer in base_model_7.layers:
    layer.trainable = False

# Unfreeze the last few layers
for layer in base_model_7.layers[-20:]:
    layer.trainable = True


# Build the model
model = Sequential()
model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))
model.add(base_model_7)
model.add(Flatten())
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(len(unique_labels), activation='softmax'))
model.summary()


# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])


# Train the model
batch_size = 20
steps_8 = int(len(train_paths) / batch_size)
epochs = 8
history_8 = model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),
                    epochs=epochs, steps_per_epoch=steps_8)


# Plot training history
plt.figure(figsize=(8, 4))
plt.grid(True)
plt.plot(history_8.history['sparse_categorical_accuracy'], 'b-', linewidth=2, label='Accuracy')
plt.plot(history_8.history['loss'], 'r-', linewidth=2, label='Loss')
plt.title('Xception Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.xticks(range(len(history_8.history['sparse_categorical_accuracy'])))
plt.legend(loc='upper right')
plt.show()


# Evaluate the model on the test set
batch_size = 32
steps_8 = int(len(test_paths) / batch_size)
y_pred_7 = []
y_true_7 = []
for x, y in tqdm(datagen(test_paths, test_labels, batch_size=batch_size, epochs=1), total=steps_8):
    pred_7 = model.predict(x)
    pred_7 = np.argmax(pred_7, axis=-1)
    for i in decode_label(pred_7):
        y_pred_7.append(i)
    for i in decode_label(y):
        y_true_7.append(i)


# Classification report
print(classification_report(y_true_7, y_pred_7))





from tensorflow.keras.applications import NASNetLarge


# Load the base model
base_model_8 = NASNetLarge(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet')

# Freeze all layers
for layer in base_model_8.layers:
    layer.trainable = False

# Unfreeze the last few layers
for layer in base_model_8.layers[-20:]:
    layer.trainable = True


# Build the model
model = Sequential()
model.add(Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))
model.add(base_model_8)
model.add(Flatten())
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(len(unique_labels), activation='softmax'))
model.summary()


# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_accuracy'])


# Train the model
batch_size = 32
steps_9 = int(len(train_paths) / batch_size)
epochs = 8
history_9 = model.fit(datagen(train_paths, train_labels, batch_size=batch_size, epochs=epochs),
                    epochs=epochs, steps_per_epoch=steps_9)



# Plot training history
plt.figure(figsize=(8, 4))
plt.grid(True)
plt.plot(history_9.history['sparse_categorical_accuracy'], 'b-', linewidth=2, label='Accuracy')
plt.plot(history_9.history['loss'], 'r-', linewidth=2, label='Loss')
plt.title('NasNet Model Training History')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.xticks(range(len(history_9.history['sparse_categorical_accuracy'])))
plt.legend(loc='upper right')
plt.show()


# Evaluate the model on the test set
batch_size = 32
steps_9 = int(len(test_paths) / batch_size)
y_pred_9 = []
y_true_9 = []
for x, y in tqdm(datagen(test_paths, test_labels, batch_size=batch_size, epochs=1), total=steps_9):
    pred_9 = model.predict(x)
    pred_9 = np.argmax(pred_9, axis=-1)
    for i in decode_label(pred_9):
        y_pred_9.append(i)
    for i in decode_label(y):
        y_true_9.append(i)


# Classification report
print(classification_report(y_true_9, y_pred_9))






